# -*- coding: utf-8 -*-
"""Copy_of_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1769ljTUDUw4OvzbaZ1h1tZ2zFnkg_w0r
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.neighbors import KNeighborsClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC

"""**Load and Explore the Dataset**"""

file_path = '/content/breast-cancer.csv'  # Dataset path
df = pd.read_csv(file_path)

# Remove any unnamed columns (e.g., 'Unnamed: 32')
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Display dataset information
print("Dataset Overview:")
df.info()

pd.set_option('display.max_columns', None)
df.head()

df.describe()

""" **Data Cleaning and Transformation**"""

columns_with_small_values = ['concave points_mean', 'concavity_mean', 'concavity_se']  # Adjust as necessary

# Apply log transformation to small values
for column in columns_with_small_values:
    if column in df.columns:
        df[column] = np.log1p(df[column])  # log1p ensures log(1 + x)

# Visualize box plots of transformed columns
print("Box plots for small-value columns:")
for column in columns_with_small_values:
    if column in df.columns:
        sns.boxplot(x=df[column])
        plt.title(f'Box Plot: {column}')
        plt.show()

# Convert categorical target to numeric
df['diagnosis'] = df['diagnosis'].map({'B': 0, 'M': 1})

# Handle missing values
numeric_columns = df.select_dtypes(include=np.number).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

def wishker(col):  #Outlier treatment
  q1,q3 = np.percentile(col,[25,75])
  iqr = q3 - q1
  lbound = q1 - (iqr * 1.5)
  ubound = q3 + (iqr * 1.5)
  return lbound,ubound
columns = df.select_dtypes(include="number").columns.drop(["class","su"], errors = "ignore").to_list()
for x in columns:
  lbound,ubound = wishker(df[x])
  df[x] = np.where(df[x] < lbound, lbound, df[x])
  df[x] = np.where(df[x] > ubound, ubound , df[x])
  sns.boxplot(data = df, x = x)
  plt.show()

"""**Correlation Analysis**"""

print("Correlation Matrix:")
target_column = 'diagnosis'
numeric_df = df.select_dtypes(include=[np.number])  # Select numeric columns
correlation_matrix = numeric_df.drop(columns=[target_column], axis=1, errors='ignore').corr() # Calculates the pairwise Pearson correlation coefficients between numeric columns.

# Plot the correlation heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, cmap='PiYG', cbar=True, fmt='.2f', vmax = 1, vmin = -1)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""**Feature and Target Separation**"""

target_column = 'diagnosis'

# Step 1: Calculate the absolute correlation with the target column
correlations = df.corr()[target_column].drop(target_column)  # Exclude target's self-correlation
correlations = correlations.abs()  # Convert to positive correlations

# Step 2: Sort correlations in descending order
sorted_correlations = correlations.sort_values(ascending=False)

# Step 3: Select features based on threshold (optional)
threshold = 0.3
selected_features = sorted_correlations[sorted_correlations > threshold]

# Display results
print("Features strongly correlated with the target:")
print(selected_features)

X = df[selected_features.index]  # Select only the features that are strongly correlated with the target
y = df[target_column]  # Target labels (e.g., 'diagnosis')

"""**Train-Test Split and Scaling**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. Feature scaling using MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"Original feature count: {X_train.shape[1]}")
print(f"Reduced feature count after PCA: {X_train_pca.shape[1]}")

"""**Random Forest Classifier**

"""

rf_clf = RandomForestClassifier(
    n_estimators=5,
    max_depth=3,
    min_samples_split=100,
    min_samples_leaf=50,
    random_state=42
)

# Fit the model on the training data
rf_clf.fit(X_train_pca, y_train)

# Predictions and probabilities
y_pred = rf_clf.predict(X_test_pca)
y_pred_proba = rf_clf.predict_proba(X_test_pca)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data: {accuracy * 100:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'M'], yticklabels=['B', 'M'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

param_grid = {
    'n_estimators': [100, 150],  # Reduced number of trees
    'max_depth': [None, 10],  # Limited depth
    'min_samples_split': [2, 5],  # Reduced split options
    'min_samples_leaf': [1, 2],  # Limited leaf options
    'bootstrap': [True]  # Use only bootstrap samples (True)
}

# Initialize the RandomForestClassifier
rf_clf = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV with fewer cross-validation folds (cv=3) for faster results
grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid,
                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train_pca, y_train)

# Get the best hyperparameters from the grid search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_rf_clf = grid_search.best_estimator_

# Predictions and probabilities using the best model
y_pred = best_rf_clf.predict(X_test_pca)
y_pred_proba = best_rf_clf.predict_proba(X_test_pca)[:, 1]

# Evaluate the tuned model
accuracy1 = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data (after hyperparameter tuning): {accuracy1*100:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'M'], yticklabels=['B', 'M'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""**K-Nearest Neighbors Classifier**"""

knn_clf = KNeighborsClassifier(n_neighbors=100)
knn_clf.fit(X_train_pca, y_train)

# Make predictions
y_pred = knn_clf.predict(X_test_pca)
y_pred_proba = knn_clf.predict_proba(X_test_pca)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data : {accuracy * 100:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'M'], yticklabels=['B', 'M'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15, 20, 25],  # A wider range of neighbors
    'weights': ['uniform', 'distance'],  # Weight function used in prediction
    'p': [1, 2, 3],  # Power parameter for the Minkowski metric (1 = Manhattan, 2 = Euclidean, 3 = Chebyshev)
    'metric': ['minkowski', 'euclidean', 'manhattan'],  # Additional metrics to try
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']  # Different algorithms to compute nearest neighbors
}

# Initialize KNeighborsClassifier
knn_clf = KNeighborsClassifier()

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(estimator=knn_clf, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train_pca, y_train)

# Get the best hyperparameters from the grid search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_knn_clf = grid_search.best_estimator_

# Predictions and probabilities using the best model
y_pred = best_knn_clf.predict(X_test_pca)
y_pred_proba = best_knn_clf.predict_proba(X_test_pca)[:, 1]

# Evaluate the tuned model
accuracy3 = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data (after hyperparameter tuning): {accuracy3 * 100:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'M'], yticklabels=['B', 'M'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""**Support Vector Machine Classifier**"""

svm_clf = SVC(probability=True, random_state=42, C=1, kernel='poly', gamma='auto')
svm_clf.fit(X_train_pca, y_train)

# Make predictions and get probabilities
y_pred = svm_clf.predict(X_test_pca)
y_pred_proba = svm_clf.predict_proba(X_test_pca)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data (lower accuracy): {accuracy * 100:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'M'], yticklabels=['B', 'M'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],          # Regularization parameter
    'kernel': ['linear', 'rbf', 'poly'],    # Kernel type
    'gamma': ['scale', 'auto', 0.1, 1, 10]  # Kernel coefficient
}

# Initialize the Support Vector Machine Classifier
svm_clf = SVC(probability=True, random_state=42)

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(estimator=svm_clf, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train_pca, y_train)

# Get the best hyperparameters from the grid search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_svm_clf = grid_search.best_estimator_

# Predictions and probabilities using the best model
y_pred = best_svm_clf.predict(X_test_pca)
y_pred_proba = best_svm_clf.predict_proba(X_test_pca)[:, 1]

# Evaluate the tuned model
accuracy2 = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Data (after hyperparameter tuning): {accuracy2 * 100:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'M'], yticklabels=['B', 'M'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""**Accuracy Comparison of Tuned Models**"""

# Plot the comparison
plt.figure(figsize=(10, 6))
plt.bar(
    ['KNN Model (Tuned)', 'Random Forest Model (Tuned)', 'SVM Model (Tuned)'],
    [accuracy3, accuracy1, accuracy2],
    color=['blue', 'green', 'orange']
)

plt.ylabel('Accuracy')
plt.title('Comparison of Tuned Model Accuracies')
plt.ylim(0, 1)
plt.tight_layout()
plt.xticks(rotation=45, ha='right')
plt.show()